{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "torch.manual_seed(777)\n",
    "from gensim.models import Word2Vec\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import f1_score,confusion_matrix,accuracy_score,precision_score,recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from pprint import pprint\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../pickle_data/charlevel.pickle\", \"rb\") as f:\n",
    "    token_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['char_chat', 'idx2char', 'char2idx', 'target'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment=token_data['char_chat']\n",
    "output=token_data['target']\n",
    "word2idx=token_data['char2idx']\n",
    "idx2word=token_data['idx2char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1=[]\n",
    "question_list=[]\n",
    "for i in comment:\n",
    "    for k in i:\n",
    "        li1.append(word2idx[k])\n",
    "    \n",
    "    question_list.append(li1)\n",
    "    li1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec=Word2Vec.load('../word2vec_model/char_model_200.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\n",
      "1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = []\n",
    "\n",
    "for word in word2idx.keys():\n",
    "    try:\n",
    "        embedding_matrix.append(model_word2vec[word])\n",
    "    except:\n",
    "        embedding_matrix.append(np.zeros(200))\n",
    "\n",
    "print(len(word2idx))\n",
    "print(len(embedding_matrix))\n",
    "\n",
    "embedding_matrix = torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_net_1d(nn.Module):\n",
    "    def __init__(self,out_channels,num_class,input_size,embedding_matrix,filter_list):\n",
    "        \n",
    "        super(Conv_net_1d,self).__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding.from_pretrained(embeddings=embedding_matrix)\n",
    "        self.filter_1= nn.Sequential(\n",
    "                            nn.Conv1d(in_channels=1,stride=200,padding=0,kernel_size=filter_list[0]*input_size,out_channels=out_channels),\n",
    "                            nn.BatchNorm1d(out_channels),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        self.filter_2= nn.Sequential(\n",
    "                            nn.Conv1d(in_channels=1,stride=200,padding=0,kernel_size=filter_list[1]*input_size,out_channels=out_channels),\n",
    "                            nn.BatchNorm1d(out_channels),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        self.filter_3= nn.Sequential(\n",
    "                            nn.Conv1d(in_channels=1,stride=200,padding=0,kernel_size=filter_list[2]*input_size,out_channels=out_channels),\n",
    "                            nn.BatchNorm1d(out_channels),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        \n",
    "#        self.dropout=nn.Dropout(0.9)\n",
    "        self.fc=nn.Linear(out_channels*len(filter_list),num_class)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.embed(x)\n",
    "        \n",
    "        x_cat=x.reshape(batch_size, 1, -1)\n",
    "        \n",
    "        out=[self.filter_1(x_cat),self.filter_2(x_cat),self.filter_3(x_cat)]\n",
    "        \n",
    "        out=[F.max_pool1d(conv,(conv.size(2),)).squeeze(2) for conv in out]\n",
    "        \n",
    "        out=torch.cat(out,1)\n",
    "        \n",
    " #       out=self.dropout(out)\n",
    "        \n",
    "        out=self.fc(out)\n",
    "        \n",
    "        out=F.softmax(out,dim=1)\n",
    "        \n",
    "        return out\n",
    "            \n",
    "    def predict(self,x,test_batch_size):\n",
    "    \n",
    "        x=self.embed(x)\n",
    "        x_cat=x.reshape(test_batch_size, 1, -1)\n",
    "        \n",
    "        out=[self.filter_1(x_cat),self.filter_2(x_cat),self.filter_3(x_cat)]\n",
    "        out=[F.max_pool1d(conv,(conv.size(2),)).squeeze(2) for conv in out]\n",
    "        \n",
    "        out = torch.cat(out, 1)\n",
    "        out = self.fc(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(question_list,output,test_size=0.26,random_state=777,stratify=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = Dataset(np.array(X_train), y_train)\n",
    "test_data = Dataset(np.array(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33615"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "        X_batch, y_batch = zip(*batch)\n",
    "        max_seq_length = max([len(x) for x in X_batch])\n",
    "        if max_seq_length < max(filter_list):\n",
    "            max_seq_length = max(filter_list)\n",
    "\n",
    "        res = []\n",
    "        for seq in X_batch:\n",
    "            if len(seq) < max_seq_length:\n",
    "                pad_seq = torch.LongTensor(seq + [0]*(max_seq_length-len(seq)))\n",
    "                res.append(pad_seq)\n",
    "            else:\n",
    "                res.append(torch.LongTensor(seq))\n",
    "        return torch.cat(res).reshape(batch_size, max_seq_length), torch.LongTensor(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Conv_net_1d(embedding_matrix=embedding_matrix,filter_list=[3,3,4],num_class=2,out_channels=25,input_size=200).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.003,weight_decay=1e-4) # use L2-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list=[3,3,4]\n",
    "filter_sizes=[3,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=27*5\n",
    "test_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_data, batch_size=27*5, shuffle=True, collate_fn=pad_sequence)\n",
    "test_loader=DataLoader(dataset=test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Update finished! ===========\n",
      "epoch: 0\n",
      "train_acc: 0.780 (26218/33615)\n",
      "train_f1: 0.125\n",
      "--------------------------\n",
      "val_acc: 0.774 (9139/11812)\n",
      "val_f1: 0.312\n",
      "avg_train_loss: 0.532\n",
      "avg_val_loss: 0.534\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 1\n",
      "train_acc: 0.794 (26686/33615)\n",
      "train_f1: 0.229\n",
      "--------------------------\n",
      "val_acc: 0.783 (9245/11812)\n",
      "val_f1: 0.349\n",
      "avg_train_loss: 0.516\n",
      "avg_val_loss: 0.525\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 2\n",
      "train_acc: 0.803 (26996/33615)\n",
      "train_f1: 0.321\n",
      "--------------------------\n",
      "val_acc: 0.788 (9312/11812)\n",
      "val_f1: 0.374\n",
      "avg_train_loss: 0.507\n",
      "avg_val_loss: 0.520\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 3\n",
      "train_acc: 0.811 (27251/33615)\n",
      "train_f1: 0.378\n",
      "--------------------------\n",
      "val_acc: 0.788 (9302/11812)\n",
      "val_f1: 0.386\n",
      "avg_train_loss: 0.500\n",
      "avg_val_loss: 0.520\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 4\n",
      "train_acc: 0.815 (27387/33615)\n",
      "train_f1: 0.405\n",
      "--------------------------\n",
      "val_acc: 0.790 (9329/11812)\n",
      "val_f1: 0.379\n",
      "avg_train_loss: 0.495\n",
      "avg_val_loss: 0.517\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 5\n",
      "train_acc: 0.822 (27642/33615)\n",
      "train_f1: 0.444\n",
      "--------------------------\n",
      "val_acc: 0.782 (9232/11812)\n",
      "val_f1: 0.414\n",
      "avg_train_loss: 0.488\n",
      "avg_val_loss: 0.525\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 6\n",
      "train_acc: 0.827 (27802/33615)\n",
      "train_f1: 0.473\n",
      "--------------------------\n",
      "val_acc: 0.786 (9282/11812)\n",
      "val_f1: 0.426\n",
      "avg_train_loss: 0.484\n",
      "avg_val_loss: 0.521\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 7\n",
      "train_acc: 0.833 (27998/33615)\n",
      "train_f1: 0.500\n",
      "--------------------------\n",
      "val_acc: 0.782 (9240/11812)\n",
      "val_f1: 0.420\n",
      "avg_train_loss: 0.479\n",
      "avg_val_loss: 0.523\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 8\n",
      "train_acc: 0.835 (28085/33615)\n",
      "train_f1: 0.511\n",
      "--------------------------\n",
      "val_acc: 0.778 (9187/11812)\n",
      "val_f1: 0.420\n",
      "avg_train_loss: 0.476\n",
      "avg_val_loss: 0.526\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 9\n",
      "train_acc: 0.840 (28235/33615)\n",
      "train_f1: 0.532\n",
      "--------------------------\n",
      "val_acc: 0.786 (9286/11812)\n",
      "val_f1: 0.420\n",
      "avg_train_loss: 0.473\n",
      "avg_val_loss: 0.520\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 10\n",
      "train_acc: 0.846 (28427/33615)\n",
      "train_f1: 0.554\n",
      "--------------------------\n",
      "val_acc: 0.780 (9210/11812)\n",
      "val_f1: 0.427\n",
      "avg_train_loss: 0.468\n",
      "avg_val_loss: 0.527\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 11\n",
      "train_acc: 0.848 (28496/33615)\n",
      "train_f1: 0.562\n",
      "--------------------------\n",
      "val_acc: 0.784 (9259/11812)\n",
      "val_f1: 0.442\n",
      "avg_train_loss: 0.466\n",
      "avg_val_loss: 0.523\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 12\n",
      "train_acc: 0.850 (28581/33615)\n",
      "train_f1: 0.571\n",
      "--------------------------\n",
      "val_acc: 0.777 (9183/11812)\n",
      "val_f1: 0.428\n",
      "avg_train_loss: 0.463\n",
      "avg_val_loss: 0.527\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 13\n",
      "train_acc: 0.854 (28694/33615)\n",
      "train_f1: 0.586\n",
      "--------------------------\n",
      "val_acc: 0.778 (9186/11812)\n",
      "val_f1: 0.440\n",
      "avg_train_loss: 0.460\n",
      "avg_val_loss: 0.529\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 14\n",
      "train_acc: 0.857 (28809/33615)\n",
      "train_f1: 0.597\n",
      "--------------------------\n",
      "val_acc: 0.783 (9252/11812)\n",
      "val_f1: 0.434\n",
      "avg_train_loss: 0.457\n",
      "avg_val_loss: 0.524\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 15\n",
      "train_acc: 0.859 (28890/33615)\n",
      "train_f1: 0.606\n",
      "--------------------------\n",
      "val_acc: 0.766 (9049/11812)\n",
      "val_f1: 0.446\n",
      "avg_train_loss: 0.455\n",
      "avg_val_loss: 0.538\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 16\n",
      "train_acc: 0.862 (28973/33615)\n",
      "train_f1: 0.618\n",
      "--------------------------\n",
      "val_acc: 0.779 (9199/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.452\n",
      "avg_val_loss: 0.528\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 17\n",
      "train_acc: 0.863 (29024/33615)\n",
      "train_f1: 0.624\n",
      "--------------------------\n",
      "val_acc: 0.785 (9272/11812)\n",
      "val_f1: 0.430\n",
      "avg_train_loss: 0.450\n",
      "avg_val_loss: 0.522\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 18\n",
      "train_acc: 0.867 (29142/33615)\n",
      "train_f1: 0.635\n",
      "--------------------------\n",
      "val_acc: 0.775 (9154/11812)\n",
      "val_f1: 0.439\n",
      "avg_train_loss: 0.448\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 19\n",
      "train_acc: 0.865 (29088/33615)\n",
      "train_f1: 0.629\n",
      "--------------------------\n",
      "val_acc: 0.777 (9177/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.449\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 20\n",
      "train_acc: 0.868 (29193/33615)\n",
      "train_f1: 0.640\n",
      "--------------------------\n",
      "val_acc: 0.776 (9162/11812)\n",
      "val_f1: 0.439\n",
      "avg_train_loss: 0.446\n",
      "avg_val_loss: 0.530\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 21\n",
      "train_acc: 0.870 (29233/33615)\n",
      "train_f1: 0.646\n",
      "--------------------------\n",
      "val_acc: 0.782 (9241/11812)\n",
      "val_f1: 0.435\n",
      "avg_train_loss: 0.445\n",
      "avg_val_loss: 0.523\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 22\n",
      "train_acc: 0.872 (29326/33615)\n",
      "train_f1: 0.655\n",
      "--------------------------\n",
      "val_acc: 0.779 (9205/11812)\n",
      "val_f1: 0.448\n",
      "avg_train_loss: 0.442\n",
      "avg_val_loss: 0.527\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 23\n",
      "train_acc: 0.872 (29329/33615)\n",
      "train_f1: 0.656\n",
      "--------------------------\n",
      "val_acc: 0.778 (9195/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.442\n",
      "avg_val_loss: 0.527\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 24\n",
      "train_acc: 0.874 (29383/33615)\n",
      "train_f1: 0.659\n",
      "--------------------------\n",
      "val_acc: 0.780 (9215/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.441\n",
      "avg_val_loss: 0.527\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 25\n",
      "train_acc: 0.875 (29400/33615)\n",
      "train_f1: 0.662\n",
      "--------------------------\n",
      "val_acc: 0.772 (9124/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.440\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 26\n",
      "train_acc: 0.878 (29500/33615)\n",
      "train_f1: 0.671\n",
      "--------------------------\n",
      "val_acc: 0.784 (9258/11812)\n",
      "val_f1: 0.427\n",
      "avg_train_loss: 0.438\n",
      "avg_val_loss: 0.523\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 27\n",
      "train_acc: 0.876 (29437/33615)\n",
      "train_f1: 0.667\n",
      "--------------------------\n",
      "val_acc: 0.779 (9201/11812)\n",
      "val_f1: 0.444\n",
      "avg_train_loss: 0.439\n",
      "avg_val_loss: 0.528\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 28\n",
      "train_acc: 0.877 (29464/33615)\n",
      "train_f1: 0.670\n",
      "--------------------------\n",
      "val_acc: 0.780 (9219/11812)\n",
      "val_f1: 0.446\n",
      "avg_train_loss: 0.438\n",
      "avg_val_loss: 0.525\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 29\n",
      "train_acc: 0.879 (29557/33615)\n",
      "train_f1: 0.678\n",
      "--------------------------\n",
      "val_acc: 0.781 (9221/11812)\n",
      "val_f1: 0.441\n",
      "avg_train_loss: 0.435\n",
      "avg_val_loss: 0.525\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 30\n",
      "train_acc: 0.878 (29524/33615)\n",
      "train_f1: 0.674\n",
      "--------------------------\n",
      "val_acc: 0.777 (9177/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.436\n",
      "avg_val_loss: 0.530\n",
      "==========================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Update finished! ===========\n",
      "epoch: 31\n",
      "train_acc: 0.879 (29541/33615)\n",
      "train_f1: 0.676\n",
      "--------------------------\n",
      "val_acc: 0.778 (9184/11812)\n",
      "val_f1: 0.451\n",
      "avg_train_loss: 0.435\n",
      "avg_val_loss: 0.529\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 32\n",
      "train_acc: 0.879 (29559/33615)\n",
      "train_f1: 0.678\n",
      "--------------------------\n",
      "val_acc: 0.776 (9171/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.435\n",
      "avg_val_loss: 0.528\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 33\n",
      "train_acc: 0.881 (29608/33615)\n",
      "train_f1: 0.682\n",
      "--------------------------\n",
      "val_acc: 0.777 (9182/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.434\n",
      "avg_val_loss: 0.529\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 34\n",
      "train_acc: 0.881 (29622/33615)\n",
      "train_f1: 0.685\n",
      "--------------------------\n",
      "val_acc: 0.778 (9184/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.433\n",
      "avg_val_loss: 0.528\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 35\n",
      "train_acc: 0.882 (29635/33615)\n",
      "train_f1: 0.686\n",
      "--------------------------\n",
      "val_acc: 0.775 (9149/11812)\n",
      "val_f1: 0.428\n",
      "avg_train_loss: 0.433\n",
      "avg_val_loss: 0.530\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 36\n",
      "train_acc: 0.882 (29662/33615)\n",
      "train_f1: 0.690\n",
      "--------------------------\n",
      "val_acc: 0.774 (9148/11812)\n",
      "val_f1: 0.434\n",
      "avg_train_loss: 0.432\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 37\n",
      "train_acc: 0.886 (29773/33615)\n",
      "train_f1: 0.700\n",
      "--------------------------\n",
      "val_acc: 0.773 (9133/11812)\n",
      "val_f1: 0.437\n",
      "avg_train_loss: 0.429\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 38\n",
      "train_acc: 0.884 (29719/33615)\n",
      "train_f1: 0.695\n",
      "--------------------------\n",
      "val_acc: 0.774 (9143/11812)\n",
      "val_f1: 0.431\n",
      "avg_train_loss: 0.431\n",
      "avg_val_loss: 0.532\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 39\n",
      "train_acc: 0.884 (29722/33615)\n",
      "train_f1: 0.694\n",
      "--------------------------\n",
      "val_acc: 0.772 (9121/11812)\n",
      "val_f1: 0.438\n",
      "avg_train_loss: 0.430\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 40\n",
      "train_acc: 0.885 (29759/33615)\n",
      "train_f1: 0.697\n",
      "--------------------------\n",
      "val_acc: 0.783 (9252/11812)\n",
      "val_f1: 0.415\n",
      "avg_train_loss: 0.429\n",
      "avg_val_loss: 0.524\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 41\n",
      "train_acc: 0.885 (29749/33615)\n",
      "train_f1: 0.697\n",
      "--------------------------\n",
      "val_acc: 0.776 (9166/11812)\n",
      "val_f1: 0.440\n",
      "avg_train_loss: 0.430\n",
      "avg_val_loss: 0.530\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 42\n",
      "train_acc: 0.886 (29783/33615)\n",
      "train_f1: 0.699\n",
      "--------------------------\n",
      "val_acc: 0.773 (9127/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.429\n",
      "avg_val_loss: 0.532\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 43\n",
      "train_acc: 0.886 (29780/33615)\n",
      "train_f1: 0.699\n",
      "--------------------------\n",
      "val_acc: 0.772 (9123/11812)\n",
      "val_f1: 0.436\n",
      "avg_train_loss: 0.429\n",
      "avg_val_loss: 0.534\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 44\n",
      "train_acc: 0.886 (29767/33615)\n",
      "train_f1: 0.700\n",
      "--------------------------\n",
      "val_acc: 0.772 (9124/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.429\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 45\n",
      "train_acc: 0.888 (29843/33615)\n",
      "train_f1: 0.707\n",
      "--------------------------\n",
      "val_acc: 0.761 (8991/11812)\n",
      "val_f1: 0.429\n",
      "avg_train_loss: 0.427\n",
      "avg_val_loss: 0.541\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 46\n",
      "train_acc: 0.888 (29849/33615)\n",
      "train_f1: 0.707\n",
      "--------------------------\n",
      "val_acc: 0.759 (8971/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.427\n",
      "avg_val_loss: 0.545\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 47\n",
      "train_acc: 0.888 (29855/33615)\n",
      "train_f1: 0.708\n",
      "--------------------------\n",
      "val_acc: 0.774 (9142/11812)\n",
      "val_f1: 0.421\n",
      "avg_train_loss: 0.427\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 48\n",
      "train_acc: 0.888 (29866/33615)\n",
      "train_f1: 0.708\n",
      "--------------------------\n",
      "val_acc: 0.769 (9078/11812)\n",
      "val_f1: 0.441\n",
      "avg_train_loss: 0.426\n",
      "avg_val_loss: 0.536\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 49\n",
      "train_acc: 0.890 (29922/33615)\n",
      "train_f1: 0.715\n",
      "--------------------------\n",
      "val_acc: 0.769 (9083/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.425\n",
      "avg_val_loss: 0.535\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 50\n",
      "train_acc: 0.889 (29876/33615)\n",
      "train_f1: 0.709\n",
      "--------------------------\n",
      "val_acc: 0.769 (9087/11812)\n",
      "val_f1: 0.431\n",
      "avg_train_loss: 0.426\n",
      "avg_val_loss: 0.537\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 51\n",
      "train_acc: 0.890 (29909/33615)\n",
      "train_f1: 0.711\n",
      "--------------------------\n",
      "val_acc: 0.775 (9156/11812)\n",
      "val_f1: 0.435\n",
      "avg_train_loss: 0.425\n",
      "avg_val_loss: 0.531\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 52\n",
      "train_acc: 0.889 (29894/33615)\n",
      "train_f1: 0.712\n",
      "--------------------------\n",
      "val_acc: 0.773 (9125/11812)\n",
      "val_f1: 0.440\n",
      "avg_train_loss: 0.426\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 53\n",
      "train_acc: 0.890 (29922/33615)\n",
      "train_f1: 0.713\n",
      "--------------------------\n",
      "val_acc: 0.768 (9077/11812)\n",
      "val_f1: 0.431\n",
      "avg_train_loss: 0.425\n",
      "avg_val_loss: 0.536\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 54\n",
      "train_acc: 0.888 (29852/33615)\n",
      "train_f1: 0.709\n",
      "--------------------------\n",
      "val_acc: 0.774 (9145/11812)\n",
      "val_f1: 0.441\n",
      "avg_train_loss: 0.426\n",
      "avg_val_loss: 0.532\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 55\n",
      "train_acc: 0.890 (29928/33615)\n",
      "train_f1: 0.716\n",
      "--------------------------\n",
      "val_acc: 0.770 (9091/11812)\n",
      "val_f1: 0.440\n",
      "avg_train_loss: 0.424\n",
      "avg_val_loss: 0.537\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 56\n",
      "train_acc: 0.888 (29855/33615)\n",
      "train_f1: 0.708\n",
      "--------------------------\n",
      "val_acc: 0.764 (9028/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.426\n",
      "avg_val_loss: 0.539\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 57\n",
      "train_acc: 0.892 (29968/33615)\n",
      "train_f1: 0.720\n",
      "--------------------------\n",
      "val_acc: 0.784 (9259/11812)\n",
      "val_f1: 0.415\n",
      "avg_train_loss: 0.424\n",
      "avg_val_loss: 0.523\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 58\n",
      "train_acc: 0.891 (29945/33615)\n",
      "train_f1: 0.718\n",
      "--------------------------\n",
      "val_acc: 0.772 (9116/11812)\n",
      "val_f1: 0.425\n",
      "avg_train_loss: 0.424\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 59\n",
      "train_acc: 0.893 (30006/33615)\n",
      "train_f1: 0.721\n",
      "--------------------------\n",
      "val_acc: 0.772 (9113/11812)\n",
      "val_f1: 0.431\n",
      "avg_train_loss: 0.422\n",
      "avg_val_loss: 0.533\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 60\n",
      "train_acc: 0.893 (30028/33615)\n",
      "train_f1: 0.723\n",
      "--------------------------\n",
      "val_acc: 0.765 (9036/11812)\n",
      "val_f1: 0.445\n",
      "avg_train_loss: 0.422\n",
      "avg_val_loss: 0.539\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 61\n",
      "train_acc: 0.893 (30003/33615)\n",
      "train_f1: 0.721\n",
      "--------------------------\n",
      "val_acc: 0.762 (9002/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.422\n",
      "avg_val_loss: 0.541\n",
      "==========================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Update finished! ===========\n",
      "epoch: 62\n",
      "train_acc: 0.891 (29962/33615)\n",
      "train_f1: 0.717\n",
      "--------------------------\n",
      "val_acc: 0.762 (8995/11812)\n",
      "val_f1: 0.445\n",
      "avg_train_loss: 0.423\n",
      "avg_val_loss: 0.543\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 63\n",
      "train_acc: 0.892 (29988/33615)\n",
      "train_f1: 0.719\n",
      "--------------------------\n",
      "val_acc: 0.769 (9079/11812)\n",
      "val_f1: 0.443\n",
      "avg_train_loss: 0.423\n",
      "avg_val_loss: 0.537\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 64\n",
      "train_acc: 0.891 (29964/33615)\n",
      "train_f1: 0.719\n",
      "--------------------------\n",
      "val_acc: 0.752 (8885/11812)\n",
      "val_f1: 0.448\n",
      "avg_train_loss: 0.423\n",
      "avg_val_loss: 0.551\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 65\n",
      "train_acc: 0.893 (30021/33615)\n",
      "train_f1: 0.723\n",
      "--------------------------\n",
      "val_acc: 0.757 (8945/11812)\n",
      "val_f1: 0.439\n",
      "avg_train_loss: 0.421\n",
      "avg_val_loss: 0.546\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n",
      "epoch: 66\n",
      "train_acc: 0.894 (30050/33615)\n",
      "train_f1: 0.724\n",
      "--------------------------\n",
      "val_acc: 0.766 (9046/11812)\n",
      "val_f1: 0.433\n",
      "avg_train_loss: 0.421\n",
      "avg_val_loss: 0.539\n",
      "==========================================================\n",
      "\n",
      "\n",
      "========= Update finished! ===========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1470a5019e84>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, test_batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx_cat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1253\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(100):\n",
    "    train_correct = 0\n",
    "    train_count = 0\n",
    "    train_loss = 0\n",
    "    val_correct = 0\n",
    "    val_count = 0\n",
    "    val_loss = 0   \n",
    "    \n",
    "        \n",
    "    train_y_pred=[]\n",
    "    train_y_true=[]\n",
    "    \n",
    "    val_y_pred=[]\n",
    "    val_y_true=[]\n",
    "    \n",
    "    # Training\n",
    "    model = model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs = Variable(X_batch).to(device)\n",
    "        targets = Variable(y_batch).to(device)\n",
    "        \n",
    "        preds = model(inputs)\n",
    "     \n",
    "        loss = criterion(preds, targets)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        compare_list=[]\n",
    "        target_list=[]\n",
    "        \n",
    "        compare = torch.max(preds, 1)[1]\n",
    "        \n",
    "        compare_list.append([i.item() for i in compare])\n",
    "        compare_list=compare_list[0]\n",
    "        target_list.append([i.item() for i in targets])\n",
    "        target_list=target_list[0]\n",
    "        \n",
    "        train_correct+=torch.sum(compare==targets).item()\n",
    "        train_count += X_batch.size(0)\n",
    "        \n",
    "        train_y_pred.extend(compare_list)\n",
    "        train_y_true.extend(target_list)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = train_correct/train_count\n",
    "    avg_train_loss = train_loss/(train_count/batch_size)\n",
    " #   print(confusion_matrix(y_pred=train_y_pred,y_true=train_y_true))\n",
    "    f1=f1_score(y_pred=train_y_pred,y_true=train_y_true)\n",
    "    \n",
    "    print('========= Update finished! ===========')\n",
    "    \n",
    "    model = model.eval()\n",
    "    with torch.no_grad():          \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            val_count += 1         \n",
    "            if len(X_batch) < max(filter_list):\n",
    "                X_batch = torch.LongTensor(X_batch + [word2idx.get(\"<pad>\")]*(max(filter_sizes)-len(X_batch))).to(device)\n",
    "            else:\n",
    "                X_batch = torch.LongTensor(X_batch).to(device)\n",
    "            inputs = Variable(X_batch).to(device)\n",
    "            target = Variable(torch.LongTensor(y_batch)).to(device)\n",
    "            pred = model.predict(inputs, test_batch_size=1)\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            test_pred_list=[]\n",
    "            test_true_list=[]\n",
    "            \n",
    "            _, pred = torch.max(pred, 1)\n",
    "            true = y_batch.item()\n",
    "            if true == pred.item():\n",
    "                val_correct +=1\n",
    "                          \n",
    "            test_pred_list.append([i.item() for i in pred])\n",
    "            test_pred_list=test_pred_list[0]\n",
    "            test_true_list.append([i.item() for i in target])\n",
    "            test_true_list=test_true_list[0]\n",
    "            \n",
    "            val_y_pred.extend(test_pred_list)\n",
    "            val_y_true.extend(test_true_list)\n",
    "            \n",
    "        val_acc = val_correct/val_count\n",
    "        avg_val_loss = val_loss/val_count\n",
    "        \n",
    "        f1_val=f1_score(y_pred=val_y_pred,y_true=val_y_true)\n",
    "        \n",
    "    if epoch % 1 == 0 or epoch == (10-1):\n",
    "  #      plt.show()\n",
    "  #      plt.figure(figsize=(10,5))\n",
    "  #      sns.heatmap(confusion_matrix(y_pred=train_y_pred,y_true=train_y_true),cmap='summer',annot=True)\n",
    "        \n",
    "        print('epoch: {:d}'.format(epoch))\n",
    "        print('train_acc: {:.3f} ({:d}/{:d})'.format(train_acc, train_correct, train_count))\n",
    "        \n",
    "        print('train_f1: {:.3f}' .format(f1))\n",
    "        print('--------------------------')\n",
    "        \n",
    "  #      sns.heatmap(confusion_matrix(y_pred=val_y_pred,y_true=val_y_true),cmap='summer',annot=True)\n",
    "        print('val_acc: {:.3f} ({:d}/{:d})'.format(val_acc, val_correct, val_count))\n",
    "        print('val_f1: {:.3f}' .format(f1_val))\n",
    "        print('avg_train_loss: {:.3f}'.format(avg_train_loss))        \n",
    "        print('avg_val_loss: {:.3f}'.format(avg_val_loss))\n",
    "        print(\"==========================================================\")\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max score= 0.484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
