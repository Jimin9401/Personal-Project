{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from konlpy.tag import Okt\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(777)\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./pickle_data/balanced_data.pickle\", \"rb\") as f:\n",
    "    balenced_data = pickle.load(f)\n",
    "    \n",
    "with open(\"./pickle_data/token_data.pickle\", \"rb\") as f:\n",
    "    token_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = balenced_data[\"comment\"]\n",
    "target = balenced_data[\"target\"]\n",
    "reviews_idx = token_data[\"comment_ix\"]\n",
    "word2idx = token_data[\"word2ix\"]\n",
    "idx2word = token_data[\"ix2word\"]\n",
    "max_seq_length = token_data[\"max_seq_length\"]\n",
    "idx2target={1:'highlight',0:'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i,k in zip(comment,target):\n",
    "    a.append([i,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in range(0,len(comment)):\n",
    "    if a[i][1]==1:\n",
    "        b.append(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['킹/Noun', '킹민철/Noun']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9932"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_non=[]\n",
    "for k in range(0,10622):\n",
    "    target_non.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10622"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9932, 10622]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ecc1060e3c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomment_non\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_non\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9932, 10622]"
     ]
    }
   ],
   "source": [
    "X_train,comment_non,y_train,y_test=train_test_split(li,target_non,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2rate={}\n",
    "for i,k in zip(rating,rating_index):\n",
    "    if k not in idx2rate:\n",
    "        idx2rate[k]=i\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_review=[]\n",
    "token_review+=[okt.morphs(i) for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_token=[]\n",
    "len_token+=[len(i) for i in token_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=-1\n",
    "for i in token_review:\n",
    "    k=len(i)\n",
    "    n+=1\n",
    "    while max(len_token)-k>0:\n",
    "        token_review[n].append('<pad>')\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "a+=[ k for i in token_review for k in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word=list(set(a))\n",
    "word_index=lb.fit_transform(bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx={}\n",
    "idx2word={}\n",
    "for i,k in zip(bag_of_word,word_index):\n",
    "    word2idx[i]=k\n",
    "    idx2word[k]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag=[]\n",
    "for i in token_review:\n",
    "    li=[]\n",
    "    \n",
    "    for k in i:\n",
    "        li.append(word2idx[k])\n",
    "    bag.append(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(bag,rating_index,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class makeData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data \n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=makeData(X_data=np.array(X_train),y_data=y_train)\n",
    "test_data=makeData(X_data=np.array(X_test),y_data=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1278"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_data,batch_size=142,shuffle=True)\n",
    "test_loader=DataLoader(dataset=test_data,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Clf(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,num_class,num_layers,hidden_size,bidirectional,num_directions):\n",
    "        \n",
    "        super(LSTM_Clf,self).__init__()\n",
    "        \n",
    "        # 안에 하이퍼 파라미터의 종류들 선언\n",
    "        \n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size=hidden_size\n",
    "        self.input_size=input_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.num_class=num_class\n",
    "        self.num_directions=num_directions\n",
    "        \n",
    "        self.embedding=nn.Embedding(input_size,\n",
    "                                    embedding_size)\n",
    "        \n",
    "        self.lstm=nn.LSTM(input_size=self.embedding_size,\n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers,\n",
    "                        batch_first=True,\n",
    "                        bidirectional=bidirectional) # 해당 sequence의 앞 뒤를 모두 고려해서 반영\n",
    "        \n",
    "        self.linear=nn.Linear(in_features=self.hidden_size*self.num_directions,out_features=self.num_class)\n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        hidden,cell=self.init_hidden(batch_size)\n",
    "        \n",
    "        embed=self.embedding(inputs)\n",
    "        \n",
    "        out, _ =self.lstm(embed,(hidden,cell))\n",
    "        \n",
    "        return self.linear(out[:, -1:, :].squeeze(1))\n",
    "    \n",
    "    \n",
    "    def predict(self,inputs):\n",
    "        \n",
    "        hidden,cell = self.init_hidden(1)\n",
    "        embed = self.embedding(inputs)\n",
    "\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        out, _ = self.lstm(embed, (hidden,cell))\n",
    "        out=self.linear(out[:, -1:, :].squeeze(1))\n",
    "        \n",
    "        return torch.round(out)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        hidden=Variable(torch.zeros(self.num_layers*self.num_directions,batch_size, self.hidden_size)).to(device)\n",
    "        cell=Variable(torch.zeros(self.num_layers*self.num_directions,batch_size, self.hidden_size)).to(device)\n",
    "        \n",
    "        return hidden,cell\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_Clf(input_size=7974,num_directions=1, embedding_size = 200,hidden_size=max(len_token),\n",
    "                num_layers=3,num_class=1,bidirectional=False).to(device)\n",
    "\n",
    "loss_function = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01,weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected torch.FloatTensor (got torch.LongTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected torch.FloatTensor (got torch.LongTensor)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCH=15\n",
    "batch_size=142\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    test_pred=[]\n",
    "    test_true=[]\n",
    "    \n",
    "    model.train()\n",
    "    for k,(X_batch,y_batch) in enumerate(train_loader):\n",
    "       \n",
    "        model.zero_grad()\n",
    "        inputs=Variable(torch.Tensor(X_batch.long())).to(device)\n",
    "        targets=Variable(torch.Tensor(y_batch.float())).to(device)\n",
    "        \n",
    "        y_pred=model(inputs)\n",
    "        \n",
    "        loss=loss_function(y_pred,targets)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (k+1) % 1 ==0:\n",
    "            print('epoch: [{}/{}] Step [{}/{}], Loss: {:.4f}'\n",
    "                .format((epoch+1),EPOCH,(k+1),len(train_loader),loss))\n",
    "    print('\\n')        \n",
    "    print('=====update finished!=====')\n",
    "          \n",
    "    test_pred=[]\n",
    "    test_true=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k,(X_batch,y_batch) in enumerate(test_loader):\n",
    "            inputs=Variable(X_batch).to(device)\n",
    "            targets=Variable(y_batch).to(device)\n",
    "\n",
    "            y_pred=model.predict(inputs)\n",
    "            y_pred=torch.max(y_pred,1)[1]\n",
    "            test_pred.append(y_pred.item())\n",
    "            test_true.append(targets.item())\n",
    "\n",
    "        print('test_acc: %.3f'%(accuracy_score(y_true=test_true,y_pred=test_pred)))\n",
    "        print('test_f1 %.3f' %(f1_score(y_true=test_true,y_pred=test_pred)))\n",
    "        print('\\n') \n",
    "\n",
    "\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
