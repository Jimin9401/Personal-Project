{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"token_data.pickle\", \"rb\") as f:\n",
    "    token_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = token_data[\"comment\"]\n",
    "target = token_data[\"target\"]\n",
    "reviews_idx = token_data[\"comment_ix\"]\n",
    "word2idx = token_data[\"word2ix\"]\n",
    "idx2word = token_data[\"ix2word\"]\n",
    "max_seq_length = token_data[\"max_seq_length\"]\n",
    "idx2target={1:'highlight',0:'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(token):\n",
    "    token_post=token.copy()\n",
    "    len_sentence=[ len(i) for i in token_post]\n",
    "    max_sentence=max(len_sentence) #최대 문장길이 9591\n",
    "    n=-1\n",
    "    for i in token_post:\n",
    "        n+=1\n",
    "        k=len(token_post[n])\n",
    "        while max_sentence-k>=0:\n",
    "            token_post[n].append('<PAD>')\n",
    "            k+=1\n",
    "    return token_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment=list(map(lambda x: padding(x),  [comment]))\n",
    "comment=comment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1=[]\n",
    "comment_list=[]\n",
    "for i in comment:\n",
    "    for k in i:\n",
    "        li1.append(word2idx[k])\n",
    "    \n",
    "    comment_list.append(li1)\n",
    "    li1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(comment_list,target,test_size=0.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Clf(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,num_class,num_layers,hidden_size,bidirectional,num_directions):\n",
    "        \n",
    "        super(RNN_Clf,self).__init__()\n",
    "        \n",
    "        # 안에 하이퍼 파라미터의 종류들 선언\n",
    "        \n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_size=hidden_size\n",
    "        self.input_size=input_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.num_class=num_class\n",
    "        self.num_directions=num_directions\n",
    "        \n",
    "        self.embedding=nn.Embedding(input_size,\n",
    "                                    embedding_size,padding_idx=word2idx['<PAD>'])\n",
    "        \n",
    "        self.rnn=nn.RNN(input_size=embedding_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=num_layers,\n",
    "                        batch_first=True,\n",
    "                        bidirectional=bidirectional,) # 해당 sequence의 앞 뒤를 모두 고려해서 반영\n",
    "        \n",
    "        self.linear=nn.Linear(in_features=hidden_size*num_directions,out_features=num_class)\n",
    "   \n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        hidden=self.init_hidden(batch_size)\n",
    "        \n",
    "        embed=self.embedding(inputs)\n",
    "        \n",
    "        out, _ =self.rnn(embed,hidden)\n",
    "        \n",
    "        return self.linear(out[:, -1:, :].squeeze(1))\n",
    "    \n",
    "    \n",
    "    def predict(self,inputs):\n",
    "        \n",
    "        hidden = self.init_hidden(100)\n",
    "        embed = self.embedding(inputs)\n",
    "\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        out, _ = self.rnn(embed, hidden)\n",
    "\n",
    "        return self.linear(out[:, -1:, :].squeeze(1))\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(self.num_layers*self.num_directions,batch_size, self.hidden_size))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=len(X_train)\n",
    "batch_size\n",
    "#bidirectional=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17786"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6492030024528503\n",
      "0.5970116853713989\n",
      "0.5409678816795349\n",
      "0.5326482057571411\n",
      "0.5661810636520386\n"
     ]
    }
   ],
   "source": [
    "model = RNN_Clf(input_size=len(word2idx),num_directions=1, embedding_size = 30,hidden_size=len(X_train[0]),\n",
    "                num_layers=2,num_class=2,bidirectional=False).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1,momentum=0.9)\n",
    "\n",
    "for epoch in range(5):\n",
    "\n",
    "    model.zero_grad()\n",
    "    inputs = Variable(torch.LongTensor(X_train)).to(device)\n",
    "    targets = Variable(torch.LongTensor(y_train)).to(device)\n",
    "    \n",
    "    preds = model(inputs)\n",
    "    \n",
    "    loss = loss_function(preds, targets)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : ['킹/Noun', '리/Noun', '아/Exclamation', '나/Noun']\n",
      "Prediction : no\n",
      "Truth : no\n",
      "\n",
      "\n",
      "Input : ['쉔무/Noun', '새/Noun']\n",
      "Prediction : no\n",
      "Truth : no\n",
      "\n",
      "\n",
      "Accuracy : 77.55161809928246\n"
     ]
    }
   ],
   "source": [
    "list_predicted=[]\n",
    "correct = 0\n",
    "for i, seq in enumerate(X_test):\n",
    "    inputs = Variable(torch.LongTensor(seq).view(1,-1)).to(device)\n",
    "    pred = model.predict(inputs)\n",
    "    _, pred = torch.max(pred, 1)\n",
    "    list_predicted.append(pred)\n",
    "    true = y_test[i]\n",
    "    if true == pred.item():\n",
    "        correct +=1\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        input_seq = [idx2word[ix] for ix in seq if ix != word2idx['<PAD>']]\n",
    "        print(\"Input :\", input_seq)\n",
    "        print(\"Prediction :\", idx2target[pred.item()])\n",
    "        print(\"Truth :\", idx2target[y_test[i]])\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"Accuracy :\", (correct/len(X_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimmi\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,list_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb=GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.23180553521745498\n",
      "F1 score: 0.36848441073793187\n"
     ]
    }
   ],
   "source": [
    "clf_nb.fit(X_train,y_train)\n",
    "y_pred=clf_nb.predict(X_test)\n",
    "\n",
    "print('Accuracy score: %s' %accuracy_score(y_test,y_pred))\n",
    "print('F1 score: %s' %f1_score(y_test,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
